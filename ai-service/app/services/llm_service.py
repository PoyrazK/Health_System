"""
LLM Service - Google Gemini Integration
Provides diagnosis assistance and patient-friendly health explanations
"""

from typing import Dict, Any, List, Optional
import os
import json

# Try to import Google Generative AI, but make it optional
try:
    import google.generativeai as genai
    GENAI_AVAILABLE = True
except ImportError:
    genai = None
    GENAI_AVAILABLE = False
    print("âš ï¸ google-generativeai not installed. LLM features will use fallback mode.")

# Configure Gemini
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")

class LLMService:
    def __init__(self):
        self.model = None
        if GENAI_AVAILABLE and GEMINI_API_KEY:
            genai.configure(api_key=GEMINI_API_KEY)
            self.model = genai.GenerativeModel('gemini-1.5-flash')
    
    async def generate_diagnosis_report(
        self,
        patient_data: Dict[str, Any],
        symptoms: List[str],
        ml_predictions: List[Dict[str, Any]],
        risk_assessment: Dict[str, Any]
    ) -> Dict[str, str]:
        """
        Generate a detailed diagnosis report for doctors
        Combines ML predictions with LLM reasoning
        """
        if not self.model:
            return self._fallback_report(ml_predictions, risk_assessment)
        
        prompt = f"""
Sen deneyimli bir dahiliye uzmanÄ±sÄ±n ve doktorlara Ã¶n deÄŸerlendirme yapan bir AI asistanÄ±sÄ±n.
AÅŸaÄŸÄ±daki hasta verilerini analiz et ve doktora yardÄ±mcÄ± bir rapor hazÄ±rla.

## Hasta Bilgileri
- YaÅŸ: {patient_data.get('age', 'Bilinmiyor')}
- Cinsiyet: {patient_data.get('gender', 'Bilinmiyor')}
- BMI: {patient_data.get('bmi', 'Bilinmiyor')}

## Semptomlar
{', '.join(symptoms)}

## ML Model Tahminleri (Makine Ã–ÄŸrenimi SonuÃ§larÄ±)
{json.dumps(ml_predictions, ensure_ascii=False, indent=2)}

## Risk DeÄŸerlendirmesi
- Risk Skoru: {risk_assessment.get('overall_risk_score', 'N/A')}%
- Risk Seviyesi: {risk_assessment.get('risk_level', 'N/A')}
- Risk FaktÃ¶rleri: {', '.join(risk_assessment.get('risk_factors', []))}

---

LÃ¼tfen aÅŸaÄŸÄ±daki formatta bir rapor oluÅŸtur:

### 1. Ã–N DEÄERLENDÄ°RME
(ML sonuÃ§larÄ±na dayanarak en olasÄ± tanÄ±larÄ± ve nedenlerini aÃ§Ä±kla)

### 2. Ã–NCELÄ°KLÄ° MÃœDAHALE Ã–NERÄ°SÄ°
(Acil mi, rutin takip mi, ileri tetkik mi gerekli?)

### 3. Ã–NERÄ°LEN TETKÄ°KLER
(TanÄ±yÄ± netleÅŸtirmek iÃ§in hangi testler yapÄ±lmalÄ±?)

### 4. AYIRICI TANI
(Dikkat edilmesi gereken diÄŸer olasÄ± durumlar)

### 5. NOTLAR
(Ek dikkat edilecek hususlar)

NOT: Bu bir AI Ã¶n deÄŸerlendirmesidir, nihai tanÄ± doktor tarafÄ±ndan konulmalÄ±dÄ±r.
"""
        
        try:
            response = self.model.generate_content(prompt)
            report_text = response.text
            
            return {
                "status": "success",
                "report": report_text,
                "model_used": "gemini-1.5-flash",
                "disclaimer": "Bu rapor AI tarafÄ±ndan oluÅŸturulmuÅŸtur. Nihai tanÄ± ve tedavi kararÄ± doktor tarafÄ±ndan verilmelidir."
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e),
                "report": self._fallback_report(ml_predictions, risk_assessment)["report"]
            }
    
    async def generate_patient_explanation(
        self,
        lab_results: Dict[str, Any],
        patient_language_level: str = "simple"
    ) -> Dict[str, str]:
        """
        Translate complex medical lab results into patient-friendly language
        """
        if not self.model:
            return self._fallback_patient_explanation(lab_results)
        
        prompt = f"""
Sen hastalarÄ±n lab sonuÃ§larÄ±nÄ± anlayabilecekleri ÅŸekilde aÃ§Ä±klayan bir saÄŸlÄ±k danÄ±ÅŸmanÄ±sÄ±n.
AÅŸaÄŸÄ±daki lab sonuÃ§larÄ±nÄ± hasta iÃ§in anlaÅŸÄ±lÄ±r bir dilde aÃ§Ä±kla.
Panik yaratmadan, sakin ve bilgilendirici bir ton kullan.

## Lab SonuÃ§larÄ±
{json.dumps(lab_results, ensure_ascii=False, indent=2)}

---

LÃ¼tfen her deÄŸer iÃ§in ÅŸu formatta aÃ§Ä±klama yap:
- ğŸŸ¢ Normal deÄŸerler iÃ§in: "Normal aralÄ±kta" + kÄ±sa aÃ§Ä±klama
- ğŸŸ¡ Hafif anormal deÄŸerler iÃ§in: "Hafif yÃ¼ksek/dÃ¼ÅŸÃ¼k" + ne anlama geldiÄŸi + panik yapmayÄ±n mesajÄ±
- ğŸ”´ Ã–nemli anormal deÄŸerler iÃ§in: "Dikkat gerektiriyor" + doktorla gÃ¶rÃ¼ÅŸÃ¼n Ã¶nerisi

Sonunda genel bir Ã¶zet ve saÄŸlÄ±k tavsiyesi ekle.
"""
        
        try:
            response = self.model.generate_content(prompt)
            return {
                "status": "success",
                "explanation": response.text,
                "model_used": "gemini-1.5-flash"
            }
        except Exception as e:
            return self._fallback_patient_explanation(lab_results)
    
    async def check_drug_interactions(
        self,
        current_medications: List[str],
        new_medication: str
    ) -> Dict[str, Any]:
        """
        Check for potential drug interactions
        """
        if not self.model:
            return {
                "status": "fallback",
                "interactions": [],
                "warning": "LLM servisi aktif deÄŸil, manuel kontrol gerekli"
            }
        
        prompt = f"""
Sen bir klinik eczacÄ±sÄ±n. AÅŸaÄŸÄ±daki ilaÃ§ etkileÅŸimlerini kontrol et.

## Mevcut Ä°laÃ§lar
{', '.join(current_medications)}

## Eklenmek Ä°stenen Ä°laÃ§
{new_medication}

---

LÃ¼tfen ÅŸu formatta yanÄ±t ver:
1. EtkileÅŸim var mÄ±? (EVET/HAYIR/BELKÄ°)
2. EtkileÅŸim varsa risk seviyesi (KRÄ°TÄ°K/ORTA/DÃœÅÃœK)
3. Hangi ilaÃ§lar arasÄ±nda etkileÅŸim var?
4. Ne tÃ¼r bir etkileÅŸim? (Ã–rn: Kanama riski artÄ±ÅŸÄ±, Etkinlik azalmasÄ±, vb.)
5. Ã–neri (Ä°laÃ§ deÄŸiÅŸikliÄŸi, doz ayarÄ±, monitÃ¶rizasyon vb.)
"""
        
        try:
            response = self.model.generate_content(prompt)
            return {
                "status": "success",
                "analysis": response.text,
                "model_used": "gemini-1.5-flash"
            }
        except Exception as e:
            return {
                "status": "error",
                "error": str(e)
            }
    
    def _fallback_report(
        self,
        ml_predictions: List[Dict[str, Any]],
        risk_assessment: Dict[str, Any]
    ) -> Dict[str, str]:
        """Fallback when LLM is not available"""
        top_disease = ml_predictions[0] if ml_predictions else {"disease": "Bilinmiyor", "probability": 0}
        
        report = f"""
### Ã–N DEÄERLENDÄ°RME (ML BazlÄ±)

**En OlasÄ± TanÄ±:** {top_disease.get('disease', 'Bilinmiyor')} 
**GÃ¼ven OranÄ±:** {top_disease.get('probability', 0)}%

**Risk Seviyesi:** {risk_assessment.get('risk_level', 'N/A')}
**Risk Skoru:** {risk_assessment.get('overall_risk_score', 'N/A')}%

### RÄ°SK FAKTÃ–RLERÄ°
{chr(10).join(['- ' + rf for rf in risk_assessment.get('risk_factors', ['Tespit edilmedi'])])}

### Ã–NERÄ°LER
{chr(10).join(['- ' + r for r in risk_assessment.get('recommendations', ['Doktor deÄŸerlendirmesi Ã¶nerilir'])])}

---
âš ï¸ Bu basitleÅŸtirilmiÅŸ bir rapordur. LLM servisi aktif deÄŸil.
"""
        return {
            "status": "fallback",
            "report": report,
            "disclaimer": "LLM servisi aktif olmadÄ±ÄŸÄ± iÃ§in basitleÅŸtirilmiÅŸ rapor oluÅŸturuldu."
        }
    
    def _fallback_patient_explanation(self, lab_results: Dict[str, Any]) -> Dict[str, str]:
        """Fallback patient explanation"""
        explanation = "Lab sonuÃ§larÄ±nÄ±z alÄ±nmÄ±ÅŸtÄ±r.\n\n"
        for key, value in lab_results.items():
            explanation += f"â€¢ {key}: {value}\n"
        explanation += "\nDetaylÄ± aÃ§Ä±klama iÃ§in doktorunuza danÄ±ÅŸÄ±n."
        
        return {
            "status": "fallback",
            "explanation": explanation
        }


# Singleton instance
llm_service = LLMService()
