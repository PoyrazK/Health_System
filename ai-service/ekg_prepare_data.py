"""
MIT-BIH EKG Data Preparation for Model Training
Optimizes data for maximum model performance
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from imblearn.over_sampling import SMOTE
from collections import Counter
import os
import json
import joblib
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

print("=" * 80)
print("ğŸ”§ MIT-BIH VERÄ° HAZIRLAMA - OPTÄ°MAL EÄÄ°TÄ°M VERÄ°SÄ°")
print("=" * 80)

# ==========================================
# CONFIGURATION
# ==========================================

DATA_PATH = r"c:\Users\muham\OneDrive\MasaÃ¼stÃ¼\AdvanceUpHackhathon\ai-service\notebooks\ekg\MIT-BIH Arrhythmia Database.csv"
OUTPUT_DIR = r"c:\Users\muham\OneDrive\MasaÃ¼stÃ¼\AdvanceUpHackhathon\ai-service\models\ekg"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# MIT-BIH label mapping to 5 classes
# Based on AAMI standard
LABEL_MAPPING = {
    # Normal beats (N class)
    'N': 'N',      # Normal beat
    'L': 'N',      # Left bundle branch block beat
    'R': 'N',      # Right bundle branch block beat
    'e': 'N',      # Atrial escape beat
    'j': 'N',      # Nodal (junctional) escape beat
    
    # SVEB - Supraventricular Ectopic Beats (S class)
    'A': 'S',      # Atrial premature beat
    'a': 'S',      # Aberrated atrial premature beat
    'J': 'S',      # Nodal (junctional) premature beat
    'S': 'S',      # Supraventricular premature beat
    
    # VEB - Ventricular Ectopic Beats (V class)
    'V': 'V',      # Premature ventricular contraction
    'E': 'V',      # Ventricular escape beat
    
    # Fusion beats (F class)
    'F': 'F',      # Fusion of ventricular and normal beat
    
    # Unknown/Noise (Q class)
    '/': 'Q',      # Paced beat
    'f': 'Q',      # Fusion of paced and normal beat
    'Q': 'Q',      # Unclassifiable beat
    '!': 'Q',      # Ventricular flutter wave
    '+': 'Q',      # Rhythm change
    '|': 'Q',      # Isolated QRS-like artifact
    '~': 'Q',      # Signal quality change
    'x': 'Q',      # Non-conducted P-wave
}

# 5-class urgency levels
URGENCY_LEVELS = {
    'N': {'level': 0, 'name': 'Normal', 'urgency': 'DÃ¼ÅŸÃ¼k'},
    'S': {'level': 1, 'name': 'SVEB', 'urgency': 'Orta'},
    'F': {'level': 2, 'name': 'Fusion', 'urgency': 'Orta-YÃ¼ksek'},
    'V': {'level': 3, 'name': 'VEB', 'urgency': 'YÃ¼ksek'},
    'Q': {'level': 4, 'name': 'Belirsiz', 'urgency': 'DeÄŸerlendirme Gerekli'}
}

# ==========================================
# 1. DATA LOADING
# ==========================================

print("\nğŸ“‚ 1. VERÄ° YÃœKLEME")
df = pd.read_csv(DATA_PATH)
print(f"   â€¢ YÃ¼klenen Ã¶rnek: {len(df):,}")
print(f"   â€¢ Ã–zellik sayÄ±sÄ±: {len(df.columns)}")

# Identify columns
print(f"\n   SÃ¼tunlar: {df.columns.tolist()[:10]}...")

# ==========================================
# 2. LABEL PROCESSING
# ==========================================

print("\nğŸ·ï¸ 2. ETÄ°KET Ä°ÅLEME")

# Find label column
label_col = 'type' if 'type' in df.columns else df.columns[-1]
print(f"   â€¢ Etiket sÃ¼tunu: {label_col}")

# Show original distribution
print(f"\n   Orijinal daÄŸÄ±lÄ±m:")
original_dist = df[label_col].value_counts()
for label, count in original_dist.items():
    pct = count / len(df) * 100
    print(f"     {label}: {count:,} ({pct:.1f}%)")

# Map to 5 classes
print(f"\n   5-sÄ±nÄ±f mapping uygulanÄ±yor...")
df['label_5class'] = df[label_col].map(lambda x: LABEL_MAPPING.get(str(x), 'Q'))

# Show new distribution
print(f"\n   Yeni daÄŸÄ±lÄ±m (5 sÄ±nÄ±f):")
new_dist = df['label_5class'].value_counts()
for label, count in new_dist.items():
    pct = count / len(df) * 100
    urgency = URGENCY_LEVELS[label]['urgency']
    print(f"     {label} ({URGENCY_LEVELS[label]['name']}): {count:,} ({pct:.1f}%) - Aciliyet: {urgency}")

# ==========================================
# 3. FEATURE SELECTION & CLEANING
# ==========================================

print("\nğŸ”§ 3. Ã–ZELLÄ°K SEÃ‡Ä°MÄ° VE TEMÄ°ZLÄ°K")

# Select feature columns (all numeric except labels)
exclude_cols = ['record', 'type', 'label_5class']
feature_cols = [col for col in df.columns if col not in exclude_cols]

print(f"   â€¢ Ã–zellik sÃ¼tunlarÄ±: {len(feature_cols)}")
print(f"   â€¢ KullanÄ±lacak Ã¶zellikler: {feature_cols[:10]}...")

# Extract features and labels
X = df[feature_cols].values
y = df['label_5class'].values

print(f"\n   â€¢ X shape: {X.shape}")
print(f"   â€¢ y shape: {y.shape}")

# Handle any NaN values
nan_count = np.isnan(X).sum()
if nan_count > 0:
    print(f"   âš ï¸ NaN deÄŸerler bulundu: {nan_count}")
    print(f"      â†’ Ortalama ile doldurma uygulanÄ±yor...")
    col_means = np.nanmean(X, axis=0)
    nan_indices = np.where(np.isnan(X))
    X[nan_indices] = np.take(col_means, nan_indices[1])
    print(f"      âœ… NaN deÄŸerler dÃ¼zeltildi")
else:
    print(f"   âœ… NaN deÄŸer yok")

# ==========================================
# 4. FEATURE ENGINEERING (Additional)
# ==========================================

print("\nğŸ”¬ 4. EK Ã–ZELLÄ°K MÃœHENDÄ°SLÄ°ÄÄ°")

# Create additional ratio features for better discrimination
original_feature_count = X.shape[1]

# Get column indices for specific features
lead0_cols = [i for i, col in enumerate(feature_cols) if col.startswith('0_')]
lead1_cols = [i for i, col in enumerate(feature_cols) if col.startswith('1_')]

# If we have both leads, create cross-lead features
if lead0_cols and lead1_cols:
    print(f"   â€¢ 2-lead correlation Ã¶zellikleri ekleniyor...")
    
    # Create new features list
    new_features = []
    
    # RR interval ratio
    pre_rr_0_idx = feature_cols.index('0_pre-RR') if '0_pre-RR' in feature_cols else None
    post_rr_0_idx = feature_cols.index('0_post-RR') if '0_post-RR' in feature_cols else None
    
    if pre_rr_0_idx and post_rr_0_idx:
        rr_ratio = X[:, pre_rr_0_idx] / (X[:, post_rr_0_idx] + 1e-6)
        new_features.append(rr_ratio.reshape(-1, 1))
    
    # QRS interval x RR ratio (arrhythmia indicator)
    qrs_idx = feature_cols.index('0_qrs_interval') if '0_qrs_interval' in feature_cols else None
    if qrs_idx and pre_rr_0_idx:
        qrs_rr_ratio = X[:, qrs_idx] / (X[:, pre_rr_0_idx] + 1e-6)
        new_features.append(qrs_rr_ratio.reshape(-1, 1))
    
    # QT / RR ratio (corrected QT proxy)
    qt_idx = feature_cols.index('0_qt_interval') if '0_qt_interval' in feature_cols else None
    if qt_idx and pre_rr_0_idx:
        qt_rr_ratio = X[:, qt_idx] / np.sqrt(X[:, pre_rr_0_idx] + 1e-6)
        new_features.append(qt_rr_ratio.reshape(-1, 1))
    
    if new_features:
        X = np.hstack([X] + new_features)
        print(f"   â†’ {len(new_features)} yeni Ã¶zellik eklendi")

print(f"   â€¢ Final Ã¶zellik sayÄ±sÄ±: {X.shape[1]} (orijinal: {original_feature_count})")

# ==========================================
# 5. NORMALIZATION
# ==========================================

print("\nğŸ“ 5. NORMALÄ°ZASYON")

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"   â€¢ Z-score normalizasyon uygulandÄ±")
print(f"   â€¢ Mean: {X_scaled.mean():.6f}")
print(f"   â€¢ Std: {X_scaled.std():.6f}")

# Save scaler
scaler_path = os.path.join(OUTPUT_DIR, 'ekg_scaler.joblib')
joblib.dump(scaler, scaler_path)
print(f"   âœ… Scaler kaydedildi: ekg_scaler.joblib")

# ==========================================
# 6. LABEL ENCODING
# ==========================================

print("\nğŸ”¢ 6. ETÄ°KET KODLAMA")

label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

print(f"   â€¢ SÄ±nÄ±flar: {label_encoder.classes_}")
print(f"   â€¢ Kodlama: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}")

# ==========================================
# 7. TRAIN/VAL/TEST SPLIT
# ==========================================

print("\nğŸ“Š 7. VERÄ° BÃ–LME (70/15/15)")

# First split: train+val / test
X_temp, X_test, y_temp, y_test = train_test_split(
    X_scaled, y_encoded,
    test_size=0.15,
    stratify=y_encoded,
    random_state=42
)

# Second split: train / val
X_train, X_val, y_train, y_val = train_test_split(
    X_temp, y_temp,
    test_size=0.176,  # 0.15/0.85 â‰ˆ 0.176
    stratify=y_temp,
    random_state=42
)

print(f"   â€¢ Train: {len(X_train):,} Ã¶rnek ({len(X_train)/len(X_scaled)*100:.1f}%)")
print(f"   â€¢ Val:   {len(X_val):,} Ã¶rnek ({len(X_val)/len(X_scaled)*100:.1f}%)")
print(f"   â€¢ Test:  {len(X_test):,} Ã¶rnek ({len(X_test)/len(X_scaled)*100:.1f}%)")

# ==========================================
# 8. SMOTE BALANCING (Train only)
# ==========================================

print("\nâš–ï¸ 8. SMOTE SINIF DENGELEMESÄ°")

print(f"   SMOTE Ã¶ncesi train daÄŸÄ±lÄ±mÄ±:")
before_dist = Counter(y_train)
for label, count in sorted(before_dist.items()):
    class_name = label_encoder.inverse_transform([label])[0]
    print(f"     {class_name}: {count:,}")

# Apply SMOTE
smote = SMOTE(sampling_strategy='auto', random_state=42, k_neighbors=3)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print(f"\n   SMOTE sonrasÄ± train daÄŸÄ±lÄ±mÄ±:")
after_dist = Counter(y_train_balanced)
for label, count in sorted(after_dist.items()):
    class_name = label_encoder.inverse_transform([label])[0]
    print(f"     {class_name}: {count:,}")

print(f"\n   â€¢ SMOTE Ã¶ncesi: {len(X_train):,} Ã¶rnek")
print(f"   â€¢ SMOTE sonrasÄ±: {len(X_train_balanced):,} Ã¶rnek")

# ==========================================
# 9. SAVE PROCESSED DATA
# ==========================================

print("\nğŸ’¾ 9. VERÄ° KAYDETME")

# Save as numpy arrays
np.savez_compressed(
    os.path.join(OUTPUT_DIR, 'ekg_data.npz'),
    X_train=X_train_balanced,
    X_val=X_val,
    X_test=X_test,
    y_train=y_train_balanced,
    y_val=y_val,
    y_test=y_test
)
print(f"   âœ… ekg_data.npz")

# Save label mapping
label_map = {int(i): str(label) for i, label in enumerate(label_encoder.classes_)}
with open(os.path.join(OUTPUT_DIR, 'label_map.json'), 'w') as f:
    json.dump(label_map, f, indent=2)
print(f"   âœ… label_map.json")

# Save urgency mapping
with open(os.path.join(OUTPUT_DIR, 'urgency_levels.json'), 'w') as f:
    json.dump(URGENCY_LEVELS, f, indent=2, ensure_ascii=False)
print(f"   âœ… urgency_levels.json")

# Save feature names
feature_names = feature_cols + ['rr_ratio', 'qrs_rr_ratio', 'qt_rr_ratio'][:X.shape[1] - original_feature_count]
with open(os.path.join(OUTPUT_DIR, 'feature_names.json'), 'w') as f:
    json.dump(feature_names, f, indent=2)
print(f"   âœ… feature_names.json")

# Save preprocessing metadata
metadata = {
    'timestamp': datetime.now().isoformat(),
    'source_file': 'MIT-BIH Arrhythmia Database.csv',
    'original_samples': len(df),
    'train_samples': len(X_train_balanced),
    'val_samples': len(X_val),
    'test_samples': len(X_test),
    'num_features': X_train_balanced.shape[1],
    'num_classes': len(label_encoder.classes_),
    'classes': list(label_encoder.classes_),
    'smote_applied': True,
    'normalization': 'StandardScaler'
}

with open(os.path.join(OUTPUT_DIR, 'preprocessing_metadata.json'), 'w') as f:
    json.dump(metadata, f, indent=2)
print(f"   âœ… preprocessing_metadata.json")

# ==========================================
# SUMMARY
# ==========================================

print("\n" + "=" * 80)
print("âœ… VERÄ° HAZIRLAMA TAMAMLANDI")
print("=" * 80)

print(f"""
ğŸ“Š Ã–ZET:
   â€¢ Orijinal veri: {len(df):,} Ã¶rnek
   â€¢ Train (balanced): {len(X_train_balanced):,} Ã¶rnek
   â€¢ Validation: {len(X_val):,} Ã¶rnek
   â€¢ Test: {len(X_test):,} Ã¶rnek
   â€¢ Ã–zellik sayÄ±sÄ±: {X_train_balanced.shape[1]}
   â€¢ SÄ±nÄ±f sayÄ±sÄ±: {len(label_encoder.classes_)}

ğŸ¯ SINIFLAR:
   0: N (Normal) - DÃ¼ÅŸÃ¼k Aciliyet
   1: S (SVEB) - Orta Aciliyet
   2: V (VEB) - YÃ¼ksek Aciliyet
   3: F (Fusion) - Orta-YÃ¼ksek Aciliyet
   4: Q (Belirsiz) - DeÄŸerlendirme Gerekli

ğŸ“ KAYDEDLEN DOSYALAR:
   â€¢ ekg_data.npz - HazÄ±r eÄŸitim verisi
   â€¢ ekg_scaler.joblib - Normalizasyon scaler
   â€¢ label_map.json - Etiket eÅŸleme
   â€¢ urgency_levels.json - Aciliyet seviyeleri
   â€¢ feature_names.json - Ã–zellik isimleri

ğŸš€ SONRAKÄ° ADIM:
   python ai-service/ekg_train_xgboost.py
""")
