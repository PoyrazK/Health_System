"""
Healthcare AI System - Data Cleaning & Preprocessing
Dataset: Disease and symptoms dataset.csv
"""

import pandas as pd
import numpy as np
from collections import Counter

print("=" * 60)
print("ğŸ§¹ HEALTHCARE AI - VERÄ° TEMÄ°ZLEME")
print("=" * 60)

# Load dataset
df = pd.read_csv(r"c:\Users\muham\OneDrive\MasaÃ¼stÃ¼\AdvanceUpHackhathon\Disease and symptoms dataset.csv")

print(f"\nğŸ“Š BAÅLANGIÃ‡:")
print(f"  â€¢ Toplam satÄ±r: {df.shape[0]:,}")
print(f"  â€¢ Toplam sÃ¼tun: {df.shape[1]}")
print(f"  â€¢ Toplam hastalÄ±k: {df['diseases'].nunique()}")

# ============================================
# 1. Hedef sÃ¼tun kontrolÃ¼
# ============================================
target_col = 'diseases'
feature_cols = [col for col in df.columns if col != target_col]

print(f"\nğŸ¯ HEDEF SÃœTUN: '{target_col}'")
print(f"ğŸ”¬ FEATURE SAYISI: {len(feature_cols)}")

# ============================================
# 2. SÄ±nÄ±f daÄŸÄ±lÄ±mÄ± analizi
# ============================================
class_counts = df[target_col].value_counts()

print("\nğŸ“Š SINIF DAÄILIMI ANALÄ°ZÄ°:")
print(f"  Min Ã¶rnek/sÄ±nÄ±f: {class_counts.min()}")
print(f"  Max Ã¶rnek/sÄ±nÄ±f: {class_counts.max()}")
print(f"  Ortalama: {class_counts.mean():.1f}")
print(f"  Median: {class_counts.median():.1f}")

# ============================================
# 3. Nadir sÄ±nÄ±flarÄ± tespit et
# ============================================
MIN_SAMPLES = 50  # Minimum Ã¶rnek sayÄ±sÄ± eÅŸik deÄŸeri

rare_classes = class_counts[class_counts < MIN_SAMPLES]
common_classes = class_counts[class_counts >= MIN_SAMPLES]

print(f"\nâš ï¸ NADÄ°R SINIFLAR (<{MIN_SAMPLES} Ã¶rnek):")
print(f"  Nadir sÄ±nÄ±f sayÄ±sÄ±: {len(rare_classes)}")
print(f"  Nadir sÄ±nÄ±flardaki toplam satÄ±r: {rare_classes.sum():,}")
print(f"\nâœ… YETERLI SINILIFLAR (>={MIN_SAMPLES} Ã¶rnek):")
print(f"  Yeterli sÄ±nÄ±f sayÄ±sÄ±: {len(common_classes)}")
print(f"  Bu sÄ±nÄ±flardaki toplam satÄ±r: {common_classes.sum():,}")

# ============================================
# 4. Temizleme iÅŸlemi
# ============================================
print("\nğŸ§¹ TEMÄ°ZLEME Ä°ÅLEMÄ°:")

# Option A: Nadir sÄ±nÄ±flarÄ± sil
df_cleaned = df[df[target_col].isin(common_classes.index)].copy()

print(f"  Silinen satÄ±r sayÄ±sÄ±: {len(df) - len(df_cleaned):,}")
print(f"  Kalan satÄ±r sayÄ±sÄ±: {len(df_cleaned):,}")
print(f"  Kalan hastalÄ±k sayÄ±sÄ±: {df_cleaned[target_col].nunique()}")

# ============================================
# 5. Feature analizi - kullanÄ±lmayan sÃ¼tunlarÄ± tespit
# ============================================
print("\nğŸ” FEATURE ANALÄ°ZÄ°:")

# HiÃ§ kullanÄ±lmayan symptom'larÄ± bul
zero_variance_cols = []
for col in feature_cols:
    if df_cleaned[col].nunique() == 1:
        zero_variance_cols.append(col)

print(f"  Tek deÄŸerli sÃ¼tunlar: {len(zero_variance_cols)}")

# Ã‡ok nadir kullanÄ±lan symptom'lar (<%1)
low_usage_cols = []
for col in feature_cols:
    if col not in zero_variance_cols:
        usage_pct = df_cleaned[col].mean() * 100
        if usage_pct < 1:  # %1'den az kullanÄ±lÄ±yor
            low_usage_cols.append((col, usage_pct))

print(f"  Nadir kullanÄ±lan symptomlar (<%1): {len(low_usage_cols)}")

# Zero variance sÃ¼tunlarÄ± sil
if zero_variance_cols:
    df_cleaned = df_cleaned.drop(columns=zero_variance_cols)
    print(f"  Silinen tek deÄŸerli sÃ¼tunlar: {len(zero_variance_cols)}")

# ============================================
# 6. Final Ã¶zet
# ============================================
print("\n" + "=" * 60)
print("âœ… TEMÄ°ZLENMÄ°Å VERÄ° SETÄ° Ã–ZETÄ°:")
print("=" * 60)
print(f"  â€¢ SatÄ±r sayÄ±sÄ±: {df_cleaned.shape[0]:,}")
print(f"  â€¢ SÃ¼tun sayÄ±sÄ±: {df_cleaned.shape[1]}")
print(f"  â€¢ HastalÄ±k sayÄ±sÄ±: {df_cleaned[target_col].nunique()}")
print(f"  â€¢ Feature sayÄ±sÄ±: {df_cleaned.shape[1] - 1}")

# Yeni sÄ±nÄ±f daÄŸÄ±lÄ±mÄ±
new_class_counts = df_cleaned[target_col].value_counts()
print(f"\n  â€¢ Min Ã¶rnek/sÄ±nÄ±f: {new_class_counts.min()}")
print(f"  â€¢ Max Ã¶rnek/sÄ±nÄ±f: {new_class_counts.max()}")
print(f"  â€¢ Ortalama: {new_class_counts.mean():.1f}")

# ============================================
# 7. Temiz veriyi kaydet
# ============================================
output_path = r"c:\Users\muham\OneDrive\MasaÃ¼stÃ¼\AdvanceUpHackhathon\cleaned_dataset.csv"
df_cleaned.to_csv(output_path, index=False)
print(f"\nğŸ’¾ Temiz veri kaydedildi: cleaned_dataset.csv")

# ============================================
# 8. HastalÄ±k listesi kaydet
# ============================================
disease_list = df_cleaned[target_col].unique().tolist()
disease_df = pd.DataFrame({
    'disease': disease_list,
    'sample_count': [new_class_counts[d] for d in disease_list]
}).sort_values('sample_count', ascending=False)

disease_df.to_csv(r"c:\Users\muham\OneDrive\MasaÃ¼stÃ¼\AdvanceUpHackhathon\disease_list.csv", index=False)
print(f"ğŸ’¾ HastalÄ±k listesi kaydedildi: disease_list.csv")

# ============================================
# 9. Label Encoding iÃ§in mapping
# ============================================
disease_to_label = {disease: idx for idx, disease in enumerate(sorted(disease_list))}
df_cleaned['disease_label'] = df_cleaned[target_col].map(disease_to_label)

# Encoding bilgisini kaydet
encoding_df = pd.DataFrame([
    {'disease': k, 'label': v} 
    for k, v in disease_to_label.items()
])
encoding_df.to_csv(r"c:\Users\muham\OneDrive\MasaÃ¼stÃ¼\AdvanceUpHackhathon\disease_encoding.csv", index=False)
print(f"ğŸ’¾ Label encoding kaydedildi: disease_encoding.csv")

print("\nâœ… Veri temizleme tamamlandÄ±!")

# Son hali kaydet
df_cleaned.to_csv(r"c:\Users\muham\OneDrive\MasaÃ¼stÃ¼\AdvanceUpHackhathon\cleaned_dataset.csv", index=False)
