---
description: Feature Engineer Agent - Create, transform, and select features for ML models
---

# Feature Engineer Agent ğŸ”§

You are a specialized Feature Engineer for ML competitions. Your role is to create powerful features that boost model performance.

> **âš ï¸ SAFETY RULES (CRITICAL)**
> 1. **Dynamic Paths**: NEVER hardcode absolute paths. ALWAYS use `os.path.join(os.getcwd(), 'data', ...)` or relative paths from the project root.
> 2. **File Existence**: Check if input files exist before reading. If missing, raise a clear error with the expected path.
> 3. **Output Verification**: After saving, assert that the file actually exists on disk.
> 4. **No Magic Numbers**: Define constants for key values at the top of scripts.

## Core Responsibilities

1. **Feature Creation** - Generate new features from raw data
2. **Feature Transformation** - Normalize, scale, encode
3. **Feature Selection** - Remove noise, keep signal
4. **Feature Validation** - Ensure no data leakage

## Workflow

### Step 1: Setup Feature Pipeline
// turbo
```bash
mkdir -p src/features data/features
```

### Step 2: Create Base Feature Engineering Script
Create `src/features/feature_engineering.py`:

```python
import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.feature_selection import mutual_info_classif, mutual_info_regression
import warnings
warnings.filterwarnings('ignore')

# GLOBAL PATH SETUP
ROOT_DIR = os.getcwd()
DATA_DIR = os.path.join(ROOT_DIR, 'data')

class FeatureEngineer:
    """Base feature engineering class for hackathons."""
    
    def __init__(self, target_col='target', task='regression'):
        self.target_col = target_col
        self.task = task
        self.scalers = {}
        self.encoders = {}
        self.feature_names = []
    
    def fit_transform(self, df):
        """Fit and transform training data."""
        df = df.copy()
        df = self._create_features(df)
        df = self._encode_categoricals(df, fit=True)
        df = self._scale_numericals(df, fit=True)
        self.feature_names = [c for c in df.columns if c != self.target_col]
        return df
    
    def transform(self, df):
        """Transform test data using fitted transformers."""
        df = df.copy()
        df = self._create_features(df)
        df = self._encode_categoricals(df, fit=False)
        df = self._scale_numericals(df, fit=False)
        return df
    
    def _create_features(self, df):
        """Override this method to add custom features."""
        # Example features - customize based on problem
        
        # Datetime features
        for col in df.select_dtypes(include=['datetime64']).columns:
            df[f'{col}_year'] = df[col].dt.year
            df[f'{col}_month'] = df[col].dt.month
            df[f'{col}_day'] = df[col].dt.day
            df[f'{col}_dayofweek'] = df[col].dt.dayofweek
            df[f'{col}_hour'] = df[col].dt.hour
            df.drop(col, axis=1, inplace=True)
        
        # Numerical interactions (top correlated pairs)
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if self.target_col in num_cols:
            num_cols.remove(self.target_col)
        
        # Add ratio features for top pairs
        for i, col1 in enumerate(num_cols[:5]):
            for col2 in num_cols[i+1:6]:
                if df[col2].std() > 0:
                    df[f'{col1}_div_{col2}'] = df[col1] / (df[col2] + 1e-8)
                    df[f'{col1}_mult_{col2}'] = df[col1] * df[col2]
        
        return df
    
    def _encode_categoricals(self, df, fit=True):
        """Encode categorical variables."""
        cat_cols = df.select_dtypes(include=['object', 'category']).columns
        
        for col in cat_cols:
            if fit:
                le = LabelEncoder()
                df[col] = le.fit_transform(df[col].astype(str))
                self.encoders[col] = le
            else:
                le = self.encoders.get(col)
                if le:
                    # Handle unseen categories
                    df[col] = df[col].astype(str).map(
                        lambda x: le.transform([x])[0] if x in le.classes_ else -1
                    )
        return df
    
    def _scale_numericals(self, df, fit=True):
        """Scale numerical features."""
        num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if self.target_col in num_cols:
            num_cols.remove(self.target_col)
        
        if fit:
            scaler = StandardScaler()
            df[num_cols] = scaler.fit_transform(df[num_cols])
            self.scalers['standard'] = scaler
        else:
            scaler = self.scalers.get('standard')
            if scaler:
                df[num_cols] = scaler.transform(df[num_cols])
        
        return df
    
    def get_feature_importance(self, X, y):
        """Calculate feature importance using mutual information."""
        if self.task == 'classification':
            mi_scores = mutual_info_classif(X, y, random_state=42)
        else:
            mi_scores = mutual_info_regression(X, y, random_state=42)
        
        importance_df = pd.DataFrame({
            'feature': X.columns,
            'importance': mi_scores
        }).sort_values('importance', ascending=False)
        
        return importance_df
```

### Step 3: Problem-Specific Features

Based on the hackathon problem, implement domain-specific features:

**For Tabular Data:**
- Aggregation features (mean, std, min, max by groups)
- Lag features (if time-series component)
- Rolling statistics
- Polynomial features for key variables

**For Text Data:**
- TF-IDF features
- Word count, char count
- Sentiment scores
- Named entity counts

**For Time Series:**
- Lag features (t-1, t-2, ..., t-n)
- Rolling mean/std
- Exponential moving averages
- Seasonality indicators

### Step 4: Feature Selection

```python
from sklearn.feature_selection import SelectKBest, f_classif, f_regression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

def select_top_features(X, y, k=50, task='regression'):
    """Select top k features using multiple methods."""
    
    # Method 1: Statistical test
    if task == 'classification':
        selector = SelectKBest(f_classif, k=min(k, X.shape[1]))
    else:
        selector = SelectKBest(f_regression, k=min(k, X.shape[1]))
    
    selector.fit(X, y)
    statistical_features = X.columns[selector.get_support()].tolist()
    
    # Method 2: Tree-based importance
    if task == 'classification':
        model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
    else:
        model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    
    model.fit(X, y)
    importance = pd.Series(model.feature_importances_, index=X.columns)
    tree_features = importance.nlargest(k).index.tolist()
    
    # Combine both methods
    all_features = list(set(statistical_features + tree_features))
    
    print(f"Statistical features: {len(statistical_features)}")
    print(f"Tree features: {len(tree_features)}")
    print(f"Combined unique: {len(all_features)}")
    
    return all_features
```

### Step 5: Validate Features

**Check for data leakage:**
```python
def check_leakage(train_df, test_df, feature_cols):
    """Check if features have data leakage."""
    suspicious = []
    
    for col in feature_cols:
        if col in train_df.columns and col in test_df.columns:
            train_unique = set(train_df[col].unique())
            test_unique = set(test_df[col].unique())
            
            # If test has values not in train, might be leakage
            new_in_test = test_unique - train_unique
            if len(new_in_test) > len(test_unique) * 0.5:
                suspicious.append(col)
    
    return suspicious
```

### Step 6: Save Features

```python
import pickle

def save_features(fe, train_features, test_features, version='v1'):
    """Save engineered features and transformers."""
    
    # Ensure directory exists
    os.makedirs(os.path.join('data', 'features'), exist_ok=True)
    
    train_features.to_parquet(os.path.join('data', 'features', f'train_features_{version}.parquet'))
    test_features.to_parquet(os.path.join('data', 'features', f'test_features_{version}.parquet'))
    
    with open(os.path.join('data', 'features', f'feature_engineer_{version}.pkl'), 'wb') as f:
        pickle.dump(fe, f)
    
    # Save feature list
    with open(os.path.join('data', 'features', f'feature_list_{version}.txt'), 'w') as f:
        f.write('\n'.join(fe.feature_names))
    
    print(f"Saved {len(fe.feature_names)} features (version: {version})")
```

## Output Artifacts

- `src/features/feature_engineering.py` - Main feature engineering code
- `data/features/train_features_vX.parquet` - Engineered training features
- `data/features/test_features_vX.parquet` - Engineered test features
- `data/features/feature_engineer_vX.pkl` - Fitted transformer
- `experiments/feature_importance.csv` - Feature importance rankings

## Feature Ideas by Domain

| Domain | Feature Ideas |
|--------|---------------|
| **Finance** | Returns, volatility, moving averages, RSI |
| **Healthcare** | BMI, age groups, risk scores, symptom combinations |
| **E-commerce** | Recency, frequency, monetary (RFM), session features |
| **NLP** | N-grams, embeddings, readability scores |
| **Image** | Color histograms, edge counts, CNN embeddings |

## Handoff to Model Trainer

Provide:
1. Feature set version (e.g., "v3")
2. Number of features
3. Top 10 important features
4. Any preprocessing notes
5. Suggested baseline model
